# -*- coding: utf-8 -*-
"""「nltk_ipynb」.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IlLs448_YoNnMBMtJmCj5zcssSZ_H_J9
"""

# 參考程式碼
# https://www.nltk.org/book/
# https://github.com/youngmihuang/NLTK

"""# ***掛接雲端硬碟***"""

from google.colab import drive
drive.mount('/content/drive')

import nltk

nltk.download("popular")
nltk.download("all-corpora")
nltk.download("punkt")

import nltk
from nltk.book import *

# text1: Moby Dick by Herman Melville 1851
# text2: Sense and Sensibility by Jane Austen 1811
# text3: The Book of Genesis
# text4: Inaugural Address Corpus
# text5: Chat Corpus
# text6: Monty Python and the Holy Grail
# text7: Wall Street Journal
# text8: Personals Corpus
# text9: The Man Who Was Thursday by G . K . Chesterton 1908
text1

# 搜尋字詞功能
import nltk
from nltk.book import *

text3.count("lived")

# text3.concordance("lived", lines = 50)  #max = 50

text4.concordance("people", lines = 30)  #max = 30

'''
根據該詞的上下文，找到類似結構，就認定他們為近似字。假設我們現在要在 text1 裡找
monstrous 字詞，而 monstrous 會出現在 the ___ pictures 以及 a ___ size
這樣的結構當中，透過這個方法去比對，以下字詞( true、 contemptible 、 christian )
會在 text1 文本出現在一樣的結構中，就認定他們為近似字。
'''

# text1: Moby Dick by Herman Melville 1851
# text2: Sense and Sensibility by Jane Austen 1811
# text3: The Book of Genesis
# text4: Inaugural Address Corpus
# text5: Chat Corpus
# text6: Monty Python and the Holy Grail
# text7: Wall Street Journal
# text8: Personals Corpus
# text9: The Man Who Was Thursday by G . K . Chesterton 1908

import nltk
from nltk.book import *

# 近似字
# text1.similar("monstrous")

#print ("----------我是分隔線-------")

# text2.similar("monstrous")

#list
context = ["monstrous","abundant", "careful"]

text1.common_contexts(context)

'''
透過計算「相異字詞長度/總字詞長度」的值，去比較不同文本之間涵蓋詞彙的豐富程度。
以 text4 為例，透過 set(text4) ，
可以獲得 text4 文本所有的相異字詞，像是 1812 、1815 、Amendment 、Abandonment 、Above 、Accept 、Americans …等，
兩者相除後計算出來的值為 0.0623。
實際執行可以發現排序後的相異字詞，有很多皆為年份，相異字詞的內容多和法律制定有關。
'''

import nltk
from nltk.book import *

# 相異字詞
setText = set(text4) # set 集合 list_1 = ["a", "a", "b", "c"] -> set() -> ["a", "b", "c"]

print(setText)

# 相異字詞排序
tempText = sorted(setText) # ["b", "c", "a"] -> sorted() -> ["a", "b", "c"]
print(tempText)

# 定義詞彙多樣性的函數
def difference(text):
    print(len(set(text)))
    print(len(text))
    return len(set(text)) / len(text)

print(difference(text4))

# 課堂練習 比較 Tex2 跟 Text3 的詞彙豐富程度誰比較高

difference(text2)

difference(text3)

'''
延續上面跟制定法律有關的 text4 文本，
如果我們想要檢視「制定美國民主」相關的字詞出現在整篇的頻率，也就是特定字詞出現在文本的前、中、後的狀況：
'''

import nltk
from nltk.book import *

# text1: Moby Dick by Herman Melville 1851
# text2: Sense and Sensibility by Jane Austen 1811
# text3: The Book of Genesis
# text4: Inaugural Address Corpus
# text5: Chat Corpus
# text6: Monty Python and the Holy Grail
# text7: Wall Street Journal
# text8: Personals Corpus
# text9: The Man Who Was Thursday by G . K . Chesterton 1908

# 詞彙分布圖
contexts = ["citizens", "democracy", "freedom", "duties", "America", "liberty", "constitution"]
text4.dispersion_plot(contexts)

# 課堂練習 練習畫出 Text3 或其他的文本來畫出詞彙分佈圖
# text3: The Book of Genesis

"""# 一層一層剝開語料庫：你會發現，文章是由大大小小的「字詞」所組成
![alt text](http://www.busiunion.com/Uploads/5875d4483fccd.png)

---
![alt text](https://cdn-images-1.medium.com/max/1600/1*FUDaEGy5rBkKjt92llStJg.png)

# corpus：使用 NLTK 提供的語料庫
## 範例包含了 4 個語料庫：gutenberg、brown、reuters、inaugural。

![alt text](https://cdn-images-1.medium.com/max/1600/1*pgwvZxhHBFjIt4nLu35Iug.png)
"""

# 透過 fileids 可以找到該語料庫底下的文本有哪些
'''
ex. gutenberg 是第一個提供免費的網路電子書平台，
根據官方網站說明，project gutenberg 已經有超過 57,000本免費的電子書
，NLTK 的 package 僅納入部分語料。

'''
from nltk.corpus import gutenberg
gutenberg.fileids()

from nltk.corpus import brown
brown.fileids()

'''
乍看之下，上述 print 出來的結果，words 跟 sents 很像。
事實上只有在範圍是一句話的時候，words 的效果跟 sents一樣，
但若範圍擴大，在資料結構上是不一樣的： words 為一個 list ，裡面包含所有字詞； sents 是以一句話為單位包成一個 list。
'''
from nltk.corpus import gutenberg
nltk.download('punkt_tab')

# 以 gutenberg 語料庫當中的第一篇語料為例
# print(gutenberg.raw('austen-emma.txt')) # ['xxx xxx .....'] 原始內容
print(gutenberg.words('austen-emma.txt')) # ['abc', 'aaa', 'ccc' ...] 依照單字斷詞
print(gutenberg.sents('austen-emma.txt')) # ['aaa vvv rrrr ssss.', 'cccc ....', 'cccc'] 依照句子斷句

# 字詞數/句子數的計算
from nltk.corpus import gutenberg

len(gutenberg.fileids())
for fileid in gutenberg.fileids():
    num_chars = len(gutenberg.raw(fileid))   # 輸出文本原始內容
    num_words = len(gutenberg.words(fileid)) # 輸出文本單詞列表
    num_sents = len(gutenberg.sents(fileid)) # 輸出文本句子列表
    print(num_chars, num_words, num_sents, fileid)

#len() #算長度
#set() #去掉重複的字
#.lower() #把文字變小寫
#round() #四捨五入

a = "APPLE"
a.lower()

b = 3.14159
round(b)

from nltk.corpus import gutenberg

for fileid in gutenberg.fileids():  #18本
    num_chars = len(gutenberg.raw(fileid))   # 輸出文章原始內容
    num_words = len(gutenberg.words(fileid)) # 輸出文章單詞列表
    num_sents = len(gutenberg.sents(fileid)) # 輸出文章句子列表
    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))  #將w都轉成小寫


    # 計算平均字詞長度、平均句子長度、詞彙多樣性
    print(round(num_chars/num_words), #round() 四捨五入
          round(num_words/num_sents),
          num_vocab/num_words, fileid)

# 語料庫內文本的分類屬性

'''
brown 語料庫是第一個百萬等級的電子語料庫(英文)，
1961 年由 Brown University 所整理，這個語料庫包含的字詞來自 500 個資料源，
並參考資料源的種類做分類，例如：adventure 、news、reviews…等。

'''
from nltk.corpus import brown
brown.categories()  #所有類別

brown.fileids()   #所有文本


# 透過查找文本id的方式：得知brown有一篇文本id為：cc01
# 查詢cc01的分類
# brown.categories("cj23")

# 有時候單一文本不只有一種分類

# 第一步: 先查詢文本id
from nltk.corpus import reuters
print(reuters.fileids())    #所有的文本
print(reuters.categories())   #所有的類別

reuters.categories('test/14829')

# 在語料庫中尋找特定字詞
from nltk.corpus import brown
# 從 brown 語料庫中找尋評論類文本的字詞
print(brown.words(categories='reviews'))
print(brown.sents(categories='reviews'))

# 結合分類與特定字詞的應用 (frequency distribution)

# 使用 FreqDist() 在單一文本中計算特定字詞個數
import nltk
from nltk.corpus import brown

reviews_text = brown.words(categories='reviews')

fdist = nltk.FreqDist(w.lower() for w in reviews_text) # fdist: {can: 45, it: 20, news: 10, } fdist["can"] -> 45

modals = ['can', 'could', 'may', 'might', 'must', 'will', 'might', 'shall']

# fdist['can']

# fdist['could']

for m in modals:
    print(m + ':', fdist[m])

import nltk
from nltk.corpus import reuters

livestock_text = reuters.words(categories="livestock")

fdist = nltk.FreqDist(w.lower() for w in livestock_text)

modals = ['can', 'could', 'may', 'might', 'must', 'will', 'might', 'shall']

for m in modals:
  print(m + ":", fdist[m])

# 練習：print出will字詞的個數









# print("will:", fdist["will"])

from nltk.corpus import brown
# 使用 ConditionalFreqDist() 在多個文本中計算特定字詞個數


cfd = nltk.ConditionalFreqDist((genre, word)
for genre in brown.categories()
for word in brown.words(categories=genre))


genres = ['reviews', 'hobbies', 'romance', 'humor']   # 風格
modals = ['would', 'could', 'may', 'might', 'must', 'will', 'shall', 'should']    # 情態助動詞
cfd.tabulate(conditions=genres, samples=modals)

# 練習用 reuters 語料庫去建立 table

import nltk
from nltk.corpus import reuters

cfd = nltk.ConditionalFreqDist((genre, word)
for genre in reuters.categories()
for word in reuters.words(categories=genre))



# genres = ['interest', 'jobs', 'nickel', 'sugar']   # 風格
# modals = ['would', 'could', 'may', 'might', 'must', 'will', 'shall', 'should']    # 情態助動詞
# cfd.tabulate(conditions=genres, samples=modals)

from nltk import *
nltk.download('punkt_tab')

sentence = """At eight o'clock on Thursday morning
... Arthur didn't feel very good."""

print(sentence.split()) # 空格去斷句
print(nltk.word_tokenize(sentence))

"""### 來看一下兩種方法斷詞的結果比較

![alt text](https://cdn-images-1.medium.com/max/1600/1*b3yf4co52h7MptFa30whWA.png)
"""

# 使用 word_tokenize 可以進行斷詞，而 sent_tokenize 可以幫助我們斷句，
#斷句
from nltk.tokenize import sent_tokenize
mytext = "Hello Adam, how are you? I hope everything is going well. Today is a good day, see you dude."
data = sent_tokenize(mytext)
print(data)

#斷詞
from nltk.tokenize import word_tokenize
mytext = "Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude."
data = word_tokenize(mytext)
print(data)

#分開非英文文字

from nltk.tokenize import sent_tokenize
nltk.download('punkt_tab')

mytext = "Bonjour M. Adam, comment allez-vous? J'espère que tout va bien. Aujourd'hui est un bon jour."
data = sent_tokenize(mytext,"french")
print(data)

'''
inaugural 是歷屆美國總統就職演說的語料庫，文本的命名方式是『年份+人名』 ，
共有 60 個文本，最新一筆收錄的是 2025 年 Trump 的演說稿
'''

from nltk.corpus import inaugural
print(inaugural.fileids())  # 60個文本
print(len(inaugural.fileids()))

sent = inaugural.raw('2025-Trump.txt')
text = nltk.sent_tokenize(sent)
print(text)
sent_length = len(text)
print(sent_length)  #計算有多少sentence

from nltk.corpus import gutenberg

sent = gutenberg.raw('austen-emma.txt')
text = nltk.sent_tokenize(sent)
print(text)

#練習：計算有多少sentence












# print(len(text))

"""# ***字詞出現頻率圖表製作***"""

# 參考連結
# https://www-xray.ast.cam.ac.uk/~jss/lecture/computing/notes/out/python_123/?fbclid=IwAR1ep6exzntEUQ7UBmlbzUue7u8cX1u7MsHWopdF91xgy8pmzWaRnbrZIdw

"""

for fileid in inaugural.fileids(): # 取出各個演說稿
     for w in inaugural.words(fileid): #講稿的字詞取出
         for target in ['america', 'citizen']: #篩選字詞
             if w.lower().startswith(target): #American, America's
                 return(target, fileid[:4])
 """


# ConditionalFreqDist() 在多個文本中計算特定字詞個數

import nltk
from nltk.corpus import inaugural
import matplotlib.pyplot as plt

cfd = nltk.ConditionalFreqDist(
    (target, fileid[:4]) # [:4]取至前四位字元
    for fileid in inaugural.fileids() # 取出年份
    for w in inaugural.words(fileid)  # 歷屆文本的字詞
    for target in ['america', 'citizen'] # 篩選字詞
    if w.lower().startswith(target)) # 字詞 American’s 也能納入計算
plt.figure(figsize=(10,5))
cfd.plot()



"""# ***停用詞Stopword***"""

'''
停用詞大致分為兩類。
1)人類語言中包含的功能詞，如'the'、'is'、'at'、'which'、'on'等。
2)詞彙詞，比如'want'等，這些詞應用十分廣泛，但是對這樣的詞搜尋引擎無法保證能夠給出真正相關的搜索結果。
'''
from nltk.corpus import stopwords
from wordcloud import STOPWORDS
print(stopwords.words('english'))
print(STOPWORDS)

"""

```
# 此內容會顯示為程式碼
```

# ***以下為進階課程***

---

"""

#分析高頻率單字

from bs4 import BeautifulSoup

import urllib.request

import nltk

from nltk.corpus import stopwords
from wordcloud import STOPWORDS

# 利用爬蟲去下載電子書的文本

response = urllib.request.urlopen('http://gutenberg.net.au/ebooks01/0100021.txt')

html = response.read()

soup = BeautifulSoup(html,"html5lib")

text = soup.get_text(strip=True)

tokens = [t for t in text.split(' ')]

clean_tokens = tokens[:]

# 移除停用詞

for token in tokens:
  if token.lower() in stopwords.words('english'):
    clean_tokens.remove(token)

print(clean_tokens)

freq = nltk.FreqDist(clean_tokens)

print(freq.items())

# for key,val in freq.items():
#  print (str(key) + ':' + str(val))
freq.plot(20,cumulative=False)

#找出高頻率單字
import pandas as pd
import numpy as np
import nltk
import os
import nltk.corpus

text = "In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern side of South America"
from nltk.tokenize import word_tokenize
token = word_tokenize(text)

from nltk.probability import FreqDist
fdist = FreqDist(token)
#出現頻率最高的10個單字
fdist1 = fdist.most_common(10)
fdist1

#找出高頻率單字
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import nltk
import os
import nltk.corpus

text = "In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern side of South America"
from nltk.tokenize import word_tokenize
token = word_tokenize(text)

from nltk.probability import FreqDist
fdist = FreqDist(token)
#出現頻率最高的10個單字
fdist1 = fdist.most_common(10)


key_list = []
value_list = []

for key,val in fdist1:
  key_list.append(key)
  value_list.append(val)

x=np.arange(len(key_list))                     #產生X軸座標序列
plt.bar(x, value_list, tick_label=key_list)     #繪製長條圖
plt.title('Top 10 words')          #設定圖形標題
plt.xlabel('Words')                               #設定X軸標籤
plt.ylabel('Nums')                          #設定Y軸標籤
plt.show()

#去除字尾Stemming
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

print(stemmer.stem('working'))

from nltk.stem import SnowballStemmer
french_stemmer = SnowballStemmer('german')
print(french_stemmer.stem("Guten"))

import nltk
from nltk.tokenize import word_tokenize
#Part of speech tagging (POS)詞性標記
text = "vote to choose a particular man or a group (party) to represent them in parliament"
#Tokenize the text
tex = word_tokenize(text)
nltk.pos_tag(tex)

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import treebank

# t = treebank.parsed_sents('wsj_0001.mrg')[1]
# t.draw()

#Chunking資訊提取
text = "We saw the yellow dog"
token = word_tokenize(text)
tags = nltk.pos_tag(token)
reg = "NP: {<DT>?<JJ>*<NN>}"
a = nltk.RegexpParser(reg)
result = a.parse(tags)
print(result)

#拼字錯誤(edit-distance)
#代表著從rain變成shine這個單字需要三步驟(sain->shin->shine)
from nltk.metrics import edit_distance
edit_distance("rain","shine")